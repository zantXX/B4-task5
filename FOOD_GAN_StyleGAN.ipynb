{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NVlabs/stylegan <br>\n",
    "https://github.com/huangzh13/StyleGAN.pytorch <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NVlabs/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nohup python -u dataset_tool.py --source=../GANdata/train --dest=datasets/food10_x256_1.zip \\\n",
    "    --width=256 --height=256 >& nohup.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/space0/yamamoto-k/jupyter/notebook/B4-5/stylegan2-ada-pytorch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/host/space0/yamamoto-k/jupyter/notebook/B4-5/stylegan2-ada-pytorch'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd stylegan2-ada-pytorch\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training options:\n",
      "{\n",
      "  \"num_gpus\": 2,\n",
      "  \"image_snapshot_ticks\": 50,\n",
      "  \"network_snapshot_ticks\": 50,\n",
      "  \"metrics\": [\n",
      "    \"fid50k_full\"\n",
      "  ],\n",
      "  \"random_seed\": 0,\n",
      "  \"training_set_kwargs\": {\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"datasets/food10_x256_1.zip\",\n",
      "    \"use_labels\": false,\n",
      "    \"max_size\": 192369,\n",
      "    \"xflip\": false,\n",
      "    \"resolution\": 256\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"num_workers\": 3,\n",
      "    \"prefetch_factor\": 2\n",
      "  },\n",
      "  \"G_kwargs\": {\n",
      "    \"class_name\": \"training.networks.Generator\",\n",
      "    \"z_dim\": 512,\n",
      "    \"w_dim\": 512,\n",
      "    \"mapping_kwargs\": {\n",
      "      \"num_layers\": 2\n",
      "    },\n",
      "    \"synthesis_kwargs\": {\n",
      "      \"channel_base\": 16384,\n",
      "      \"channel_max\": 512,\n",
      "      \"num_fp16_res\": 4,\n",
      "      \"conv_clamp\": 256\n",
      "    }\n",
      "  },\n",
      "  \"D_kwargs\": {\n",
      "    \"class_name\": \"training.networks.Discriminator\",\n",
      "    \"block_kwargs\": {},\n",
      "    \"mapping_kwargs\": {},\n",
      "    \"epilogue_kwargs\": {\n",
      "      \"mbstd_group_size\": 4\n",
      "    },\n",
      "    \"channel_base\": 16384,\n",
      "    \"channel_max\": 512,\n",
      "    \"num_fp16_res\": 4,\n",
      "    \"conv_clamp\": 256\n",
      "  },\n",
      "  \"G_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"lr\": 0.0025,\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08\n",
      "  },\n",
      "  \"D_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"lr\": 0.0025,\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
      "    \"r1_gamma\": 0.4096\n",
      "  },\n",
      "  \"total_kimg\": 25000,\n",
      "  \"batch_size\": 32,\n",
      "  \"batch_gpu\": 16,\n",
      "  \"ema_kimg\": 10.0,\n",
      "  \"ema_rampup\": 0.05,\n",
      "  \"ada_target\": 0.6,\n",
      "  \"augment_kwargs\": {\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\n",
      "    \"xflip\": 1,\n",
      "    \"rotate90\": 1,\n",
      "    \"xint\": 1,\n",
      "    \"scale\": 1,\n",
      "    \"rotate\": 1,\n",
      "    \"aniso\": 1,\n",
      "    \"xfrac\": 1,\n",
      "    \"brightness\": 1,\n",
      "    \"contrast\": 1,\n",
      "    \"lumaflip\": 1,\n",
      "    \"hue\": 1,\n",
      "    \"saturation\": 1\n",
      "  },\n",
      "  \"run_dir\": \"training-runs/00000-food10_x256_1-auto2\"\n",
      "}\n",
      "\n",
      "Output directory:   training-runs/00000-food10_x256_1-auto2\n",
      "Training data:      datasets/food10_x256_1.zip\n",
      "Training duration:  25000 kimg\n",
      "Number of GPUs:     2\n",
      "Number of images:   192369\n",
      "Image resolution:   256\n",
      "Conditional model:  False\n",
      "Dataset x-flips:    False\n",
      "\n",
      "Dry run; exiting.\n"
     ]
    }
   ],
   "source": [
    "%run train.py --outdir=training-runs --data=datasets/food10_x256_1.zip --gpus=2 --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nohup python -u train.py --outdir=training-runs --data=datasets/food10_x256_1.zip --gpus=2 >& nohup_3.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nohup python -u train.py --outdir=training-runs --data=datasets/food10_x256_1.zip --gpus=2 --resume=training-runs/00000-food10_x256_1-auto2/network-snapshot-008600.pkl >& nohup_3.txt &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
