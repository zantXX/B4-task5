{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【dataset】<br>\n",
    "/export/data/dataset/UECFOOD/BB/UECFOOD100　<br>\n",
    "/export/data/dataset/UECFOOD/BB/UECFOOD256 <br>\n",
    "/export/data/dataset/food101 <br>\n",
    "\n",
    "【model】<br>\n",
    "fine-turing from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html <br>\n",
    "SOTA in Survey(reproduced) https://github.com/aurotripathy/food-classify <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuning on VGG16 on UECFOOD100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.1\n",
      "Torchvision Version:  0.6.0a0+35d732a\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"      # \"0\":GPU0, \"1\":GPU1, \"0,1\":GPUを2つとも使用\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./UECFOOD100\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg16\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 100\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.vgg16_bn(pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "        \n",
    "    elif model_name == \"vgg16\":\n",
    "        \"\"\" VGG16_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "#image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "#dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.001, betas=[0.9, 0.999])\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=16, gamma=0.5)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/31\n",
      "----------\n",
      "train Loss: 2.8617 Acc: 0.3275\n",
      "val Loss: 1.7271 Acc: 0.5409\n",
      "\n",
      "Epoch 1/31\n",
      "----------\n",
      "train Loss: 2.1841 Acc: 0.4490\n",
      "val Loss: 1.5904 Acc: 0.5708\n",
      "\n",
      "Epoch 2/31\n",
      "----------\n",
      "train Loss: 2.0169 Acc: 0.4814\n",
      "val Loss: 1.5039 Acc: 0.5879\n",
      "\n",
      "Epoch 3/31\n",
      "----------\n",
      "train Loss: 1.9686 Acc: 0.4873\n",
      "val Loss: 1.4817 Acc: 0.5969\n",
      "\n",
      "Epoch 4/31\n",
      "----------\n",
      "train Loss: 1.9471 Acc: 0.4976\n",
      "val Loss: 1.4571 Acc: 0.6001\n",
      "\n",
      "Epoch 5/31\n",
      "----------\n",
      "train Loss: 1.9097 Acc: 0.5044\n",
      "val Loss: 1.4713 Acc: 0.6015\n",
      "\n",
      "Epoch 6/31\n",
      "----------\n",
      "train Loss: 1.8859 Acc: 0.5123\n",
      "val Loss: 1.4355 Acc: 0.6129\n",
      "\n",
      "Epoch 7/31\n",
      "----------\n",
      "train Loss: 1.8798 Acc: 0.5131\n",
      "val Loss: 1.4395 Acc: 0.6046\n",
      "\n",
      "Epoch 8/31\n",
      "----------\n",
      "train Loss: 1.8573 Acc: 0.5157\n",
      "val Loss: 1.4022 Acc: 0.6154\n",
      "\n",
      "Epoch 9/31\n",
      "----------\n",
      "train Loss: 1.8795 Acc: 0.5157\n",
      "val Loss: 1.3896 Acc: 0.6248\n",
      "\n",
      "Epoch 10/31\n",
      "----------\n",
      "train Loss: 1.8505 Acc: 0.5220\n",
      "val Loss: 1.3932 Acc: 0.6272\n",
      "\n",
      "Epoch 11/31\n",
      "----------\n",
      "train Loss: 1.8320 Acc: 0.5233\n",
      "val Loss: 1.3842 Acc: 0.6297\n",
      "\n",
      "Epoch 12/31\n",
      "----------\n",
      "train Loss: 1.8445 Acc: 0.5281\n",
      "val Loss: 1.3817 Acc: 0.6397\n",
      "\n",
      "Epoch 13/31\n",
      "----------\n",
      "train Loss: 1.8048 Acc: 0.5340\n",
      "val Loss: 1.4282 Acc: 0.6192\n",
      "\n",
      "Epoch 14/31\n",
      "----------\n",
      "train Loss: 1.8273 Acc: 0.5277\n",
      "val Loss: 1.3713 Acc: 0.6286\n",
      "\n",
      "Epoch 15/31\n",
      "----------\n",
      "train Loss: 1.8075 Acc: 0.5356\n",
      "val Loss: 1.3852 Acc: 0.6304\n",
      "\n",
      "Epoch 16/31\n",
      "----------\n",
      "train Loss: 1.7402 Acc: 0.5481\n",
      "val Loss: 1.3222 Acc: 0.6391\n",
      "\n",
      "Epoch 17/31\n",
      "----------\n",
      "train Loss: 1.7116 Acc: 0.5504\n",
      "val Loss: 1.3068 Acc: 0.6429\n",
      "\n",
      "Epoch 18/31\n",
      "----------\n",
      "train Loss: 1.6846 Acc: 0.5602\n",
      "val Loss: 1.3176 Acc: 0.6446\n",
      "\n",
      "Epoch 19/31\n",
      "----------\n",
      "train Loss: 1.6780 Acc: 0.5558\n",
      "val Loss: 1.3090 Acc: 0.6422\n",
      "\n",
      "Epoch 20/31\n",
      "----------\n",
      "train Loss: 1.6546 Acc: 0.5636\n",
      "val Loss: 1.3191 Acc: 0.6429\n",
      "\n",
      "Epoch 21/31\n",
      "----------\n",
      "train Loss: 1.6651 Acc: 0.5622\n",
      "val Loss: 1.3225 Acc: 0.6432\n",
      "\n",
      "Epoch 22/31\n",
      "----------\n",
      "train Loss: 1.6297 Acc: 0.5687\n",
      "val Loss: 1.3129 Acc: 0.6478\n",
      "\n",
      "Epoch 23/31\n",
      "----------\n",
      "train Loss: 1.6755 Acc: 0.5635\n",
      "val Loss: 1.2964 Acc: 0.6526\n",
      "\n",
      "Epoch 24/31\n",
      "----------\n",
      "train Loss: 1.6434 Acc: 0.5655\n",
      "val Loss: 1.3099 Acc: 0.6526\n",
      "\n",
      "Epoch 25/31\n",
      "----------\n",
      "train Loss: 1.6433 Acc: 0.5628\n",
      "val Loss: 1.2990 Acc: 0.6533\n",
      "\n",
      "Epoch 26/31\n",
      "----------\n",
      "train Loss: 1.6304 Acc: 0.5671\n",
      "val Loss: 1.2878 Acc: 0.6537\n",
      "\n",
      "Epoch 27/31\n",
      "----------\n",
      "train Loss: 1.6251 Acc: 0.5704\n",
      "val Loss: 1.3024 Acc: 0.6478\n",
      "\n",
      "Epoch 28/31\n",
      "----------\n",
      "train Loss: 1.6109 Acc: 0.5691\n",
      "val Loss: 1.3030 Acc: 0.6519\n",
      "\n",
      "Epoch 29/31\n",
      "----------\n",
      "train Loss: 1.6524 Acc: 0.5654\n",
      "val Loss: 1.3154 Acc: 0.6425\n",
      "\n",
      "Epoch 30/31\n",
      "----------\n",
      "train Loss: 1.6452 Acc: 0.5661\n",
      "val Loss: 1.2984 Acc: 0.6460\n",
      "\n",
      "Epoch 31/31\n",
      "----------\n",
      "train Loss: 1.6486 Acc: 0.5674\n",
      "val Loss: 1.3088 Acc: 0.6544\n",
      "\n",
      "Training complete in 24m 46s\n",
      "Best val Acc: 0.654368\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存する。\n",
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_VGG16_UECFOOD100_2_1.pth\")\n",
    "\n",
    "# 保存したモデルを読み込む。\n",
    "#model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuning on VGG16 on UECFOOD256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./UECFOOD256\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg16\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 256\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "cuda:0\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.vgg16_bn(pretrained=True)\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,step_size=16,gamma=0.1)\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/31\n",
      "----------\n",
      "train Loss: 3.9010 Acc: 0.2034\n",
      "val Loss: 2.6318 Acc: 0.3913\n",
      "\n",
      "Epoch 1/31\n",
      "----------\n",
      "train Loss: 2.9034 Acc: 0.3371\n",
      "val Loss: 2.2634 Acc: 0.4535\n",
      "\n",
      "Epoch 2/31\n",
      "----------\n",
      "train Loss: 2.6801 Acc: 0.3728\n",
      "val Loss: 2.1464 Acc: 0.4678\n",
      "\n",
      "Epoch 3/31\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_VGG16_UECFOOD256_2_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tuning on VGG16 on FOOD101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./food101/images\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg16\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 101\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = models.vgg16_bn(pretrained=True)\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,step_size=16,gamma=0.1)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_VGG16_food101_2_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WISeR on UECFOOD100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/host/space0/yamamoto-k/jupyter/notebook/B4-5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/data/dataset/food101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/host/data/dataset/food101'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd food101\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/data/dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/host/data/dataset'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/space0/yamamoto-k/jupyter/notebook/B4-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/host/space0/yamamoto-k/jupyter/notebook/B4-5'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /host/space0/yamamoto-k/jupyter/notebook/B4-5\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python build_dataset.py --root-folder food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nohup python -u train_validate.py --batch-size 32 --epochs 32 --train-data ./UECFOOD100 >& nohup.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python3 test_model.py --model-file ./trained_models/checkpoint.pth --test-data /media/auro/RAID5/food-101/food-101/train_val_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = 0.5\n",
    "\n",
    "class SliceBranch(torch.nn.Module):\n",
    "    \"\"\" Describe slice branch from the paper \"\"\" \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SliceBranch, self).__init__()\n",
    "        kernel_size = (224, 5)  # \n",
    "        self.wide_conv = torch.nn.Conv2d(input_size,\n",
    "                                    output_size,\n",
    "                                    kernel_size,\n",
    "                                    stride=1,\n",
    "                                    padding=0,\n",
    "                                    bias=True)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "        self.maxpool = torch.nn.MaxPool2d((1, 5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = nn.functional.relu(self.bn(self.wide_conv(x)))\n",
    "        out2 = self.maxpool(out1)\n",
    "        out3 = self.maxpool(out2)\n",
    "        out4 = self.maxpool(out3)\n",
    "        \n",
    "        return out4\n",
    "\n",
    "class WideResnet101PlusSlice(torch.nn.Module):\n",
    "  def __init__(self, nb_classes, drop_prob):\n",
    "    super(WideResnet101PlusSlice, self).__init__()\n",
    "    self.slice_branch = SliceBranch(3, 320)\n",
    "    self.wide_res101_pretrained = models.wide_resnet101_2(pretrained=True)\n",
    "    \n",
    "    #add\n",
    "    set_parameter_requires_grad(self.wide_res101_pretrained, True)\n",
    "    \n",
    "    self.res101_branch = torch.nn.Sequential(*list(self.wide_res101_pretrained.children())[:-1])\n",
    "\n",
    "    self.fc1 = torch.nn.Linear(2368, 2048)\n",
    "    self.dropout = nn.Dropout(p=drop_prob)\n",
    "    self.fc2 = torch.nn.Linear(2048, nb_classes)  \n",
    "\n",
    "  def forward(self, x):\n",
    "    s_b = self.slice_branch(x)\n",
    "    #print(s_b.size())\n",
    "    r_b = self.res101_branch(x)\n",
    "    #print(r_b.size())\n",
    "    out = torch.cat([s_b, r_b], dim=1)    \n",
    "    out = torch.flatten(out, 1)\n",
    "    out = self.fc1(out)\n",
    "    out = self.dropout(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./UECFOOD100\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 100\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = WideResnet101PlusSlice(num_classes, drop_prob)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size= 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "#image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "#dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_ft = models.vgg16_bn(pretrained=True)\n",
    "# Initialize the model for this run\n",
    "#model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)\n",
    "\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,step_size=16,gamma=0.1)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_WISeR_UECFOOD100_2_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WISeR on UECFOOD256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./UECFOOD256\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 256\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = WideResnet101PlusSlice(num_classes, drop_prob)\n",
    "#model_ft = models.vgg16_bn(pretrained=True)\n",
    "# Initialize the model for this run\n",
    "#model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)\n",
    "\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,step_size=16,gamma=0.1)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_WISeR_UECFOOD256_2_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WISeR on FOOD101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./food101/images\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 101\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 32\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = WideResnet101PlusSlice(num_classes, drop_prob)\n",
    "#model_ft = models.vgg16_bn(pretrained=True)\n",
    "# Initialize the model for this run\n",
    "#model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)\n",
    "\n",
    "\n",
    "dataset=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['val']) \n",
    "dataset2=torchvision.datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
    "\n",
    "num = len(dataset)\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "image_datasets = {'train': trainset,\n",
    "                                  'val': testset,}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft,step_size=16,gamma=0.1)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(),\"./model_save/Classification_WISeR_FOOD101_2_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
